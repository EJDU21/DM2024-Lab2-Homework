{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30d95ac7-df81-4230-a5b0-14475d876dde",
    "_uuid": "ba75bd8c-523c-427c-819f-87bf7c23127d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:杜義榤\n",
    "\n",
    "Student ID:113232506\n",
    "\n",
    "GitHub ID:EJDU21\n",
    "\n",
    "Kaggle name:yijieeeee\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "![2024-12-05 140046.png](2024-12-05 140046.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "from IPython.display import Image\n",
    "Image('./img/pic0.png')#印出競賽排名截圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "3f851ee6-760c-4cb3-8595-07db49888244",
    "_uuid": "83ff34ba-5015-48ec-a1a8-8a939c7e1ad8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-17T08:13:20.190084Z",
     "iopub.status.busy": "2024-11-17T08:13:20.189665Z",
     "iopub.status.idle": "2024-11-17T08:13:22.313591Z",
     "shell.execute_reply": "2024-11-17T08:13:22.312381Z",
     "shell.execute_reply.started": "2024-11-17T08:13:20.190037Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "96f95f93-7826-4911-8c25-fb8dacfaeb02",
    "_uuid": "a777cc4e-042c-48a6-9632-e0859ba46828",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-17T08:13:26.606908Z",
     "iopub.status.busy": "2024-11-17T08:13:26.605997Z",
     "iopub.status.idle": "2024-11-17T08:14:03.41111Z",
     "shell.execute_reply": "2024-11-17T08:14:03.410085Z",
     "shell.execute_reply.started": "2024-11-17T08:13:26.606861Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('./dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    " \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "5202275a-255b-4453-8e65-dad7547e1b8e",
    "_uuid": "bb0f3ccb-79b6-49b2-b5b3-7201bb4c2993",
    "execution": {
     "iopub.execute_input": "2024-11-17T08:14:03.413297Z",
     "iopub.status.busy": "2024-11-17T08:14:03.412927Z",
     "iopub.status.idle": "2024-11-17T08:14:06.382892Z",
     "shell.execute_reply": "2024-11-17T08:14:06.381921Z",
     "shell.execute_reply.started": "2024-11-17T08:14:03.413258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion = pd.read_csv('./dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "data_identification = pd.read_csv('./dm-2024-isa-5810-lab-2-homework/data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a410ec8f-a5d4-4f48-b08a-a7f5bb9b9a15",
    "_uuid": "9f156694-8a51-40be-b850-9ff962eaceb3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-17T08:14:09.569126Z",
     "iopub.status.busy": "2024-11-17T08:14:09.568581Z",
     "iopub.status.idle": "2024-11-17T08:14:22.867284Z",
     "shell.execute_reply": "2024-11-17T08:14:22.866295Z",
     "shell.execute_reply.started": "2024-11-17T08:14:09.569068Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "cbe2a071-d9a2-49d6-ac3a-7fa0e13ae7f7",
    "_uuid": "fc1b1186-e0dd-4609-a96b-566e98b35285",
    "execution": {
     "iopub.execute_input": "2024-11-17T08:14:22.869316Z",
     "iopub.status.busy": "2024-11-17T08:14:22.868975Z",
     "iopub.status.idle": "2024-11-17T08:14:26.175199Z",
     "shell.execute_reply": "2024-11-17T08:14:26.174072Z",
     "shell.execute_reply.started": "2024-11-17T08:14:22.869278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')\n",
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "y_train_data = train_data['emotion']\n",
    "X_train_data = train_data.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449182\n",
      "1449182\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(X_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411972\n",
      "411972\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data.merge(emotion, on='tweet_id', how='left')\n",
    "test_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "y_test_data = test_data['emotion']\n",
    "X_test_data = test_data.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(X_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _sentencepiece: 找不到指定的程序。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspm\u001b[39;00m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\iverson\\miniconda3\\envs\\kaggle_as2\\lib\\site-packages\\sentencepiece\\__init__.py:10\u001b[0m\n",
      "\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Import the low-level C/C++ module\u001b[39;00m\n",
      "\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __package__ \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m:\n",
      "\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _sentencepiece\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_sentencepiece\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _sentencepiece: 找不到指定的程序。"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# **1. 同義詞替換**\n",
    "def synonym_replacement(sentence, n=2):\n",
    "    words = sentence.split()\n",
    "    if len(words) < n:  # 如果句子詞數量少於 n，調整 n\n",
    "        n = len(words)\n",
    "    new_sentence = words[:]\n",
    "    random_indices = random.sample(range(len(words)), n)\n",
    "    for idx in random_indices:\n",
    "        synonyms = wordnet.synsets(words[idx])\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            new_sentence[idx] = synonym\n",
    "    return ' '.join(new_sentence)\n",
    "\n",
    "# **2. 隨機刪除**\n",
    "def random_deletion(sentence, p=0.1):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return sentence  # 保證至少有一個詞\n",
    "    new_sentence = [word for word in words if random.random() > p]\n",
    "    return ' '.join(new_sentence) if new_sentence else random.choice(words)\n",
    "\n",
    "# **3. 回譯**\n",
    "def back_translate(sentence, src_lang=\"en\", tgt_lang=\"fr\"):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\")\n",
    "    model = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\")\n",
    "    translated = tokenizer.encode(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    translated = model.generate(translated, max_length=512, num_beams=4, early_stopping=True)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    # 回譯\n",
    "    tokenizer = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}\")\n",
    "    model = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}\")\n",
    "    translated = tokenizer.encode(translated_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    translated = model.generate(translated, max_length=512, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# **4. 資料增強**\n",
    "def augment_data(data, synonym_ratio=0.3, delete_ratio=0.2, backtranslate_ratio=0.3):\n",
    "    augmented_data = []\n",
    "\n",
    "    # 計算每個增強方法需要處理的數據量\n",
    "    synonym_count = int(len(data) * synonym_ratio)\n",
    "    delete_count = int(len(data) * delete_ratio)\n",
    "    backtranslate_count = int(len(data) * backtranslate_ratio)\n",
    "    original_count = len(data) - (synonym_count + delete_count + backtranslate_count)\n",
    "\n",
    "    # 隨機分配數據\n",
    "    data = data.sample(frac=1, random_state=42)  # 打亂數據\n",
    "    synonym_data = data.iloc[:synonym_count]\n",
    "    delete_data = data.iloc[synonym_count:synonym_count + delete_count]\n",
    "    backtranslate_data = data.iloc[synonym_count + delete_count:synonym_count + delete_count + backtranslate_count]\n",
    "    original_data = data.iloc[synonym_count + delete_count + backtranslate_count:]\n",
    "\n",
    "    # 應用同義詞替換\n",
    "    for text in synonym_data['text']:\n",
    "        augmented_data.append(synonym_replacement(text, n=2))\n",
    "\n",
    "    # 應用隨機刪除\n",
    "    for text in delete_data['text']:\n",
    "        augmented_data.append(random_deletion(text, p=0.1))\n",
    "\n",
    "    # 應用回譯\n",
    "    for text in backtranslate_data['text']:\n",
    "        augmented_data.append(back_translate(text, src_lang=\"en\", tgt_lang=\"fr\"))\n",
    "\n",
    "    # 保留原始數據\n",
    "    for text in original_data['text']:\n",
    "        augmented_data.append(text)\n",
    "\n",
    "    return pd.DataFrame({'text': augmented_data})\n",
    "\n",
    "# **5. 應用增強到訓練數據**\n",
    "augmented_X_train = augment_data(X_train_data, synonym_ratio=0.3, delete_ratio=0.2, backtranslate_ratio=0.3)\n",
    "\n",
    "print(augmented_X_train.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T08:35:12.803885Z",
     "iopub.status.busy": "2024-11-17T08:35:12.803387Z",
     "iopub.status.idle": "2024-11-17T08:35:13.877633Z",
     "shell.execute_reply": "2024-11-17T08:35:13.876634Z",
     "shell.execute_reply.started": "2024-11-17T08:35:12.803842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iverson\\miniconda3\\envs\\kaggle_as2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "####################################################################\n",
    "# Load Model & Tokenizer\n",
    "model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "####################################################################\n",
    "# move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "####################################################################\n",
    "#讓dataloader讀取資料\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "####################################################################\n",
    "# 分批處理\n",
    "def encode_texts_in_batches(texts, tokenizer, model, device, batch_size=32, max_length=512):\n",
    "    num_samples = len(texts)\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, num_samples, batch_size), desc=\"Encoding batches\"):\n",
    "        batch_texts = texts[i:i+batch_size].tolist()\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# word embedding\n",
    "X_train_embeddings = encode_texts_in_batches(X_train_data['text'], tokenizer, model, device, batch_size=32)\n",
    "X_test_embeddings = encode_texts_in_batches(X_test_data['text'], tokenizer, model, device, batch_size=32)\n",
    "############################################################################################################\n",
    "#save tokenizer\n",
    "save_path = \"./saved_tokenizer\"\n",
    "os.makedirs(save_path, exist_ok=True) \n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Tokenizer saved to {save_path}\")\n",
    "###################################################################\n",
    "#save embedding\n",
    "np.save('X_train_embeddings.npy', X_train_embeddings)\n",
    "np.save('X_test_embeddings.npy', X_test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE 降维\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_train_tsne = tsne.fit_transform(X_train_embeddings)\n",
    "# plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y, cmap='Spectral', s=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#load embedding\n",
    "X_train_embeddings = np.load('X_train_embeddings.npy', mmap_mode='r')\n",
    "X_test_embeddings = np.load('X_test_embeddings.npy', mmap_mode='r')\n",
    "# one hot\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x Conv2d(1, 256, kernel_size=(1, 1536), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.12batch/s, loss=1.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Average Loss: 1.232000875854172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.08batch/s, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Average Loss: 1.1887551269526406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.85batch/s, loss=1.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Average Loss: 1.1739239607324155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.43batch/s, loss=1.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Average Loss: 1.1646166129349989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.70batch/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Average Loss: 1.1574935066712617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.17batch/s, loss=1.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Average Loss: 1.1523787281924267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.60batch/s, loss=1.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Average Loss: 1.1487682381691897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.16batch/s, loss=1.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Average Loss: 1.145294524235705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.82batch/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Average Loss: 1.142254378770986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 22644/22644 [02:00<00:00, 188.29batch/s, loss=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Average Loss: 1.1393740989861398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 22644/22644 [01:56<00:00, 194.95batch/s, loss=0.936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Average Loss: 1.1370905406313763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 22644/22644 [01:56<00:00, 195.10batch/s, loss=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Average Loss: 1.135342616616174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 22644/22644 [01:56<00:00, 194.91batch/s, loss=1.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Average Loss: 1.1330721371851085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 22644/22644 [01:56<00:00, 193.61batch/s, loss=1.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Average Loss: 1.1300097828991862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.15batch/s, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Average Loss: 1.1294082000343608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.05batch/s, loss=1.77] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Average Loss: 1.1272933563960144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.40batch/s, loss=1.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Average Loss: 1.1262115835932913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 22644/22644 [02:13<00:00, 169.13batch/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Average Loss: 1.1241141223055675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 22644/22644 [02:28<00:00, 152.71batch/s, loss=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Average Loss: 1.1240766503547237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 22644/22644 [02:25<00:00, 155.16batch/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Average Loss: 1.1227924573881596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 22644/22644 [02:28<00:00, 152.84batch/s, loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Average Loss: 1.1212087313792158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 22644/22644 [02:14<00:00, 168.82batch/s, loss=0.823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Average Loss: 1.1203860701909862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.61batch/s, loss=1.56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Average Loss: 1.1193184258738416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 22644/22644 [02:03<00:00, 183.31batch/s, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Average Loss: 1.1184380024439917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.61batch/s, loss=1.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Average Loss: 1.1173805212004828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.33batch/s, loss=0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Average Loss: 1.1175997484316782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.54batch/s, loss=1.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Average Loss: 1.1165072248636296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.97batch/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Average Loss: 1.1153107213060083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.28batch/s, loss=1.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Average Loss: 1.1152047486197632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.25batch/s, loss=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Average Loss: 1.1132515006174029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.79batch/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Average Loss: 1.1144472077196719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.17batch/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Average Loss: 1.113576263160361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 22644/22644 [02:02<00:00, 185.04batch/s, loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Average Loss: 1.111223884059797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.74batch/s, loss=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Average Loss: 1.1121191338239005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.94batch/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Average Loss: 1.1115671131372282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.79batch/s, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Average Loss: 1.1105239167901615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 22644/22644 [02:01<00:00, 186.10batch/s, loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Average Loss: 1.110442685496047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 22644/22644 [02:01<00:00, 185.77batch/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Average Loss: 1.1094438251574497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.47batch/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Average Loss: 1.1095704874624277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 22644/22644 [02:02<00:00, 184.25batch/s, loss=1.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Average Loss: 1.109238948115631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 22644/22644 [02:03<00:00, 183.06batch/s, loss=1.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Average Loss: 1.1087766108138468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 22644/22644 [02:04<00:00, 181.68batch/s, loss=1.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Average Loss: 1.107471488554388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 22644/22644 [02:04<00:00, 181.86batch/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Average Loss: 1.1078413371888387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 22644/22644 [02:05<00:00, 181.08batch/s, loss=0.94] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Average Loss: 1.1059948172807863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 22644/22644 [02:04<00:00, 181.39batch/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Average Loss: 1.107225616312326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 22644/22644 [02:04<00:00, 181.77batch/s, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Average Loss: 1.1059730908586836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 22644/22644 [02:04<00:00, 182.47batch/s, loss=1.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Average Loss: 1.1059255368206067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 22644/22644 [02:04<00:00, 182.12batch/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Average Loss: 1.1066247623067003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 22644/22644 [02:04<00:00, 181.64batch/s, loss=0.923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Average Loss: 1.1055362051113653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 22644/22644 [02:04<00:00, 182.30batch/s, loss=1.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Average Loss: 1.1055227710262159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train textCnn( pytorch )\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyper parameter\n",
    "embedding_dim = X_train_embeddings.shape[1]  # word embedding 维度\n",
    "num_classes = len(set(y_train_data))\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "#讓dataloader讀取資料\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "# encode label\n",
    "y_train = le.fit_transform(y_train_data)\n",
    "\n",
    "# create train dataset\n",
    "train_dataset = EmbeddingDataset(X_train_embeddings, y_train)\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#train\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_channels=256):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_channels, (1, embedding_dim))  # model \n",
    "            for _ in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # 增加两个维度，形狀從 [batch_size, embedding_dim] -> [batch_size, 1, 1, embedding_dim]\n",
    "        x = [torch.relu(conv(x)).squeeze(3).squeeze(2) for conv in self.convs]  # (batch_size, num_channels)\n",
    "        x = torch.cat(x, 1)  \n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "model = TextCNN(embedding_dim, num_classes)\n",
    "print(model)\n",
    "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # train(w/o 進度條)\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs, targets = inputs.to(torch.device('cuda')), targets.to(torch.device('cuda'))\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # 初始化訓練進度條\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for inputs, targets in pbar:\n",
    "            # 將數據移到 GPU\n",
    "            inputs, targets = inputs.to(torch.device('cuda')), targets.to(torch.device('cuda'))\n",
    "            \n",
    "            # forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # backward & optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # 更新進度條\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # 打印當前 epoch 的平均loss\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to './save/submission_TextCNN.csv'\n"
     ]
    }
   ],
   "source": [
    "#textCnn predict\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#讓dataloader讀取資料\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "# test embedding\n",
    "test_dataset = EmbeddingDataset(X_test_embeddings)\n",
    "\n",
    "#dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 切换到评估模式\n",
    "model.eval()\n",
    "\n",
    "# 存储预测结果\n",
    "all_predictions = []\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)  # 获取预测的类别索引\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# 反编碼預測结果\n",
    "y_pred_labels = le.inverse_transform(all_predictions)\n",
    "\n",
    "# 建立輸出 DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],  # 假设 test_data 包含 tweet_id 信息\n",
    "    'emotion': y_pred_labels\n",
    "})\n",
    "\n",
    "# 保存到 CSV 文件\n",
    "submission.to_csv('./save/submission_TextCNN.csv', index=False)\n",
    "print(\"Predictions saved to './save/submission_TextCNN.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iverson\\miniconda3\\envs\\kaggle_as2\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "    (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1536, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   3%|▎         | 761/22644 [00:09<04:47, 76.15batch/s, loss=1.9] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# 累积损失\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# 更新进度条\u001b[39;00m\n\u001b[0;32m    117\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train transformer ( pytorch )\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyper parameter\n",
    "embedding_dim = X_train_embeddings.shape[1]  # 词嵌入维度\n",
    "num_classes = len(set(y_train_data))  # 类别数量\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "#讓dataloader讀取資料\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "# encode label \n",
    "y_train = le.fit_transform(y_train_data)  \n",
    "\n",
    "# create training dataset\n",
    "train_dataset = EmbeddingDataset(X_train_embeddings, y_train)\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,  # input 維度\n",
    "            nhead=num_heads,  \n",
    "            dim_feedforward=embedding_dim * 4, \n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=num_layers  # Transformer Encoder 層數\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 輸入形狀: [batch_size, embedding_dim]\n",
    "        x = x.unsqueeze(1)  # 添加一个维度 [batch_size, seq_len=1, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)# Transformer Encoder\n",
    "        x = x.mean(dim=0)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "def get_scheduler(optimizer, d_model, warmup_steps=4000):\n",
    "    def lr_lambda(step):\n",
    "        if step == 0:\n",
    "            return 1e-7\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 初始化 Transformer 模型\n",
    "model = TransformerClassifier(embedding_dim=embedding_dim, num_classes=num_classes, num_heads=8, num_layers=2)\n",
    "print(model)\n",
    "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 使用學習率调度器\n",
    "d_model = embedding_dim  # 嵌入维度\n",
    "warmup_steps = 4000\n",
    "scheduler = get_scheduler(optimizer, d_model, warmup_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # 初始化进度条\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for inputs, targets in pbar:\n",
    "            # 將數據移到 GPU\n",
    "            inputs, targets = inputs.to(torch.device('cuda')), targets.to(torch.device('cuda'))\n",
    "            \n",
    "            # forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # backward & optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # 梯度剪裁\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            # loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # 更新進度條\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    # 打印當前 epoch 的平均損失\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to './save/submission_TextCNN.csv'\n"
     ]
    }
   ],
   "source": [
    "#transformer predict (pytorch)\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#讓dataloader讀取資料\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "\n",
    "test_dataset = EmbeddingDataset(X_test_embeddings)#test embedding \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)#dataloader\n",
    "\n",
    "# 切换模型到評估模式\n",
    "model.eval()\n",
    "\n",
    "# 儲存預測结果\n",
    "all_predictions = []\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# 反編碼預測結果\n",
    "y_pred_labels = le.inverse_transform(all_predictions)\n",
    "\n",
    "# 建立 DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],\n",
    "    'emotion': y_pred_labels\n",
    "})\n",
    "\n",
    "# 保存到 CSV 文件\n",
    "submission.to_csv('./save/submission_TF.csv', index=False)\n",
    "print(\"Predictions saved to './save/submission_TextCNN.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LogisticRegression (sklearn)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf2.fit(X_train_embeddings, y_train)\n",
    "################################################################################################################\n",
    "#predict LogisticRegression (sklearn)\n",
    "y_pred_LogisticRegression = clf2.predict(X_test_embeddings)\n",
    "y_pred_labels = le.inverse_transform(y_pred_LogisticRegression)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],\n",
    "    'emotion': y_pred_labels})\n",
    "#################################################################################################################\n",
    "# 保存到 CSV 文件\n",
    "submission.to_csv('./save/submission_LogisticRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LGBMClassifier (sklearn)\n",
    "import lightgbm as lgb\n",
    "clf4 = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "clf4.fit(X_train_embeddings, y_train)\n",
    "y_pred_LGBMClassifier = clf4.predict(X_test_embeddings)\n",
    "################################################################################################################\n",
    "#predict LGBMClassifier (sklearn)\n",
    "y_pred_labels = le.inverse_transform(y_pred_LGBMClassifier)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],\n",
    "    'emotion': y_pred_labels})\n",
    "##################################################################################################################\n",
    "# 保存到 CSV 文件\n",
    "submission.to_csv('./save/submission_LGBMClassifier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train MLPClassifier (sklearn)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf6 = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42, warm_start=True)\n",
    "clf6.fit(X_train_embeddings, y_train)\n",
    "y_pred_MLPClassifier = clf6.predict(X_test_embeddings)\n",
    "################################################################################################################\n",
    "#predict MLPClassifier (sklearn)\n",
    "y_pred_labels = le.inverse_transform(y_pred_MLPClassifier)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],\n",
    "    'emotion': y_pred_labels})\n",
    "#################################################################################################################\n",
    "# 保存到 CSV 文件\n",
    "submission.to_csv('./save/submission_MLPClassifier.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9912598,
     "sourceId": 87232,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle_as2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
